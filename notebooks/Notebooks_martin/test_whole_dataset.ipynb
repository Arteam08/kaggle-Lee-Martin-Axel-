{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/martindrieux/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from autocorrect import Speller\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pycountry\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.pipeline import Pipeline\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import importlib\n",
    "\n",
    "\n",
    "import fonctions_temp_pandas\n",
    "importlib.reload(fonctions_temp_pandas)\n",
    "from fonctions_temp_pandas import *\n",
    "from time import time\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "embedding_dim = 1000\n",
    "spell=Speller(lang=\"en\", fast=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(tweets, keywords=[\"football\", \"goal\"]):\n",
    "    feature_matrix = []\n",
    "    for i, text in enumerate(tweets):\n",
    "        tokens = tokenizer(text) # ex : 'Hello, I am a test' -> ['hello', 'i', 'am', 'a', 'test']\n",
    "        token_counts = Counter(tokens)\n",
    "        len_token = max(len(tokens), 1)\n",
    "        frequencies = [token_counts.get(keyword, 0)/len_token for keyword in keywords]\n",
    "        feature_matrix.append(frequencies)\n",
    "    feature_matrix = np.array(feature_matrix)\n",
    "    return feature_matrix \n",
    "\n",
    "def fit_tfidf_embedding(train_tweets, max_features=2000, ngram_range=(1,1), max_df=0.7, min_df=2, sublinear_tf=False):\n",
    "    \"\"\"\n",
    "    Fit a TF-IDF vectorizer on the training tweets.\n",
    "    Returns the fitted vectorizer.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features,\n",
    "                                 max_df=max_df,\n",
    "                                 ngram_range=ngram_range,\n",
    "                                 min_df=min_df,\n",
    "                                 sublinear_tf=sublinear_tf)\n",
    "    vectorizer.fit(train_tweets)\n",
    "    return vectorizer\n",
    "\n",
    "def tfidf_embedding(tweets, vectorizer):\n",
    "    \"\"\"\n",
    "    Transform tweets into TF-IDF features using a fitted vectorizer.\n",
    "    \"\"\"\n",
    "    return vectorizer.transform(tweets).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data is: (1438007, 6)\n",
      "The shape of the evaluation data is: (1072928, 5)\n",
      "The shape of the training data after removing duplicates is: (1272324, 6)\n",
      "The shape of the evaluation data after removing duplicates is: (963395, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MatchID</th>\n",
       "      <th>PeriodID</th>\n",
       "      <th>EventType</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Seconds</th>\n",
       "      <th>NormalizedFrequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sami khedira staing marcelo staing best luck</td>\n",
       "      <td>0</td>\n",
       "      <td>0.607822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>damn half ya saying go wasnt even rooting begi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.607822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ones playing today course fifa couldnt let happen</td>\n",
       "      <td>0</td>\n",
       "      <td>0.607822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>injury list neymar reus gundogan gomez l bende...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.607822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>still favored vs homefield advantage mt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.607822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  MatchID  PeriodID  EventType  \\\n",
       "0  1_0        1         0          0   \n",
       "1  1_0        1         0          0   \n",
       "2  1_0        1         0          0   \n",
       "3  1_0        1         0          0   \n",
       "4  1_0        1         0          0   \n",
       "\n",
       "                                               Tweet  Seconds  \\\n",
       "0       sami khedira staing marcelo staing best luck        0   \n",
       "1  damn half ya saying go wasnt even rooting begi...        0   \n",
       "2  ones playing today course fifa couldnt let happen        0   \n",
       "3  injury list neymar reus gundogan gomez l bende...        0   \n",
       "4            still favored vs homefield advantage mt        0   \n",
       "\n",
       "   NormalizedFrequency  \n",
       "0             0.607822  \n",
       "1             0.607822  \n",
       "2             0.607822  \n",
       "3             0.607822  \n",
       "4             0.607822  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the preprocessed data\n",
    "df_train = pd.read_csv(\"df_train_withoutstemming.csv\")\n",
    "df_eval = pd.read_csv(\"df_eval_withoutstemming.csv\")\n",
    "# df_train = pd.read_csv(\"df_train_tweets_with_lemmatization.csv\")\n",
    "# df_eval = pd.read_csv(\"df_eval_tweets_with_lemmatization.csv\")\n",
    "\n",
    "# str the 'Tweet' column\n",
    "df_train['Tweet'] = df_train['Tweet'].astype(str)\n",
    "df_eval['Tweet'] = df_eval['Tweet'].astype(str)\n",
    "print(f'The shape of the training data is: {df_train.shape}')\n",
    "print(f'The shape of the evaluation data is: {df_eval.shape}')\n",
    "\n",
    "# Filter : \n",
    "df_train = df_train[df_train[\"Tweet\"].notnull()]  # Remove null values\n",
    "df_train = df_train[df_train[\"Tweet\"].str.strip() != \"\"]  # Remove empty strings\n",
    "df_eval = df_eval[df_eval[\"Tweet\"].notnull()]  # Remove null values\n",
    "df_eval = df_eval[df_eval[\"Tweet\"].str.strip() != \"\"]  # Remove empty strings\n",
    "\n",
    "# # Remove any dupplicates\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_eval = df_eval.drop_duplicates()\n",
    "\n",
    "print(f'The shape of the training data after removing duplicates is: {df_train.shape}')\n",
    "print(f'The shape of the evaluation data after removing duplicates is: {df_eval.shape}')\n",
    "\n",
    "df_train_plus = add_freq_normalized(add_time_sec(df_train))\n",
    "df_eval_plus = add_freq_normalized(add_time_sec(df_eval))\n",
    "\n",
    "df_train_plus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "output_dim = 1\n",
    "embedding_dim = 2000\n",
    "input_dim = embedding_dim +1 # +1 for the normalized frequency\n",
    "weight_1_0 = 0.55\n",
    "hidden_dims = [embedding_dim*3,embedding_dim//20, embedding_dim//10]\n",
    "dropouts = [0.95] * len(hidden_dims)\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "decay = 1e-5\n",
    "train= df_train_plus\n",
    "\n",
    "vectorizer = fit_tfidf_embedding(train['Tweet'], max_features=embedding_dim)\n",
    "\n",
    "# Transform data\n",
    "train_X = tfidf_embedding(train['Tweet'], vectorizer)\n",
    "eval_X = tfidf_embedding(df_eval_plus['Tweet'], vectorizer)\n",
    "\n",
    "train_X = np.hstack([train_X, train[['NormalizedFrequency']].values.reshape(-1, 1)])\n",
    "eval_X = np.hstack([eval_X, df_eval_plus[['NormalizedFrequency']].values.reshape(-1, 1)])#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on SVM with regularization:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "scheduler = None\n",
    "print(\"Testing on SVM with regularization:\")\n",
    "t = time()\n",
    "svm = SVC(C=0.1, kernel='linear')  # Smaller C increases regularization\n",
    "svm.fit(train_X, train['EventType'])\n",
    "\n",
    "print(f\"Model trained in {time()-t:.2f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Logistic Regression\n",
    "print(\"Testing on Logistic Regression with regularization:\")\n",
    "t = time()\n",
    "lg = LogisticRegression(max_iter=1000, C=0.1, penalty='l2', solver='liblinear')  # Stronger L2 regularization\n",
    "lg.fit(train_X, train['EventType'])\n",
    "\n",
    "print(f\"Model trained in {time()-t:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Random Forest\n",
    "print(\"Testing on Random Forest with regularization:\")\n",
    "t = time()\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=3)  # Depth/leaf regularization\n",
    "rf.fit(train_X, train['EventType'])\n",
    "\n",
    "print(f\"Model trained in {time()-t:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_preds = svm.predict(eval_X)\n",
    "\n",
    "# Logistic Regression Predictions\n",
    "lg_preds = lg.predict(eval_X)\n",
    "\n",
    "# Random Forest Predictions\n",
    "rf_preds = rf.predict(eval_X)\n",
    "\n",
    "#create dataframes with tweet ID and predictions\n",
    "df_eval_svm = pd.DataFrame(df_eval['TweetID'])\n",
    "df_eval_lg = pd.DataFrame(df_eval['TweetID'])\n",
    "df_eval_rf = pd.DataFrame(df_eval['TweetID'])\n",
    "\n",
    "df_eval_svm['Predicted'] = svm_preds\n",
    "df_eval_lg['Predicted'] = lg_preds\n",
    "df_eval_rf['Predicted'] = rf_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_kaggle2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
