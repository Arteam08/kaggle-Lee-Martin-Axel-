{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/lee.kadz/miniconda3/envs/inf554/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from time import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, verbose=False):\n",
    "    \"\"\"\n",
    "    Load and concatenate CSV files from a specified directory.\n",
    "\n",
    "    This function reads all CSV files in the given directory, concatenates them into a single DataFrame,\n",
    "    and optionally prints the first few rows and the shape of the resulting DataFrame.\n",
    "\n",
    "    Args:\n",
    "      path (str): The directory path containing the CSV files to be loaded.\n",
    "      verbose (bool, optional): If True, prints the first few rows and the shape of the concatenated DataFrame. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: A DataFrame containing the concatenated data from all CSV files in the specified directory.\n",
    "    \"\"\"\n",
    "    li = []\n",
    "    for filename in os.listdir(path):\n",
    "        df = pd.read_csv(os.path.join(path, filename))\n",
    "        li.append(df)\n",
    "    output = pd.concat(li)\n",
    "    if verbose:\n",
    "        print(output.head())\n",
    "        print(f'The shape of the data is: {output.shape}')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tweets(df, MAX_SUBGROUP=150, event_type=True):\n",
    "    \"\"\"\n",
    "    For each group of tweets with the same ID/MatchID/PeriodID/EventType,\n",
    "    We create `MAX_SUBGROUP` subgroups of tweets by concatenating them.\n",
    "    Therefore if `MAX_SUBGROUP` = 1, we have 1 tweet per ID/MatchID/PeriodID/EventType\n",
    "    If `MAX_SUBGROUP` = +inf, we have all the tweets in different subgroups.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Grouping tweets who have same timestamp by concatenating them\n",
    "    # Create an array of random integers in {0, ..., MAX_SUBGROUP} of size len(df_train)\n",
    "    df[\"random_id\"] = np.random.randint(0, MAX_SUBGROUP, len(df))\n",
    "    if event_type:\n",
    "        df_bis = df.groupby(['ID', \"MatchID\", \"PeriodID\", \"EventType\", \"random_id\"])['Tweet'].apply(lambda x: ' '.join(x)).reset_index().drop(columns='random_id')\n",
    "    else:\n",
    "        df_bis = df.groupby(['ID', \"MatchID\", \"PeriodID\", \"random_id\"])['Tweet'].apply(lambda x: ' '.join(x)).reset_index().drop(columns='random_id')\n",
    "    df_bis = df_bis.sample(frac=1).reset_index(drop=True)\n",
    "    return df_bis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Bertweet model for tweet embeddings\n",
    "model_name = \"vinai/bertweet-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_Bertweet = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_in_batches(model, tokenizer, tweets, batch_size=10, device=\"cpu\"):\n",
    "  \"\"\"\n",
    "  Extracts [CLS] token embeddings and mean-pooled token embeddings from a list of tweets in batches.\n",
    "\n",
    "  Args:\n",
    "    model (transformers.PreTrainedModel): The pre-trained transformer model to use for generating embeddings.\n",
    "    tokenizer (transformers.PreTrainedTokenizer): The tokenizer corresponding to the pre-trained model.\n",
    "    tweets (list of str): A list of tweets to process.\n",
    "    batch_size (int, optional): The number of tweets to process in each batch. Default is 100.\n",
    "    device (str, optional): The device to run the model on ('cpu' or 'cuda'). Default is 'cpu'.\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: A NumPy array containing the concatenated [CLS] and mean-pooled token embeddings for all tweets.\n",
    "  \"\"\"\n",
    "\n",
    "  embeddings = []\n",
    "  dataloader = DataLoader(tweets, batch_size=batch_size, shuffle=False)\n",
    "  model = model.to(device)\n",
    "  for batch in dataloader:\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].to(device)\n",
    "    token_embeddings = outputs.last_hidden_state[:, 1:, :]\n",
    "    attention_mask = inputs[\"attention_mask\"][:, 1:]\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "    sum_embeddings = torch.sum(token_embeddings * mask_expanded, dim=1)\n",
    "    sum_mask = torch.sum(mask_expanded, dim=1)\n",
    "    mean_pooling = sum_embeddings / sum_mask\n",
    "    combined_embeddings = torch.cat((cls_embeddings, mean_pooling), dim=1)\n",
    "    embeddings.append(combined_embeddings)\n",
    "\n",
    "  return np.concatenate([emb.cpu().numpy() for emb in embeddings], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Loading data...\n",
      "Data loaded in 0.10 seconds\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Loading data...\")\n",
    "t = time()\n",
    "path_to_data = \"../challenge_data/\"\n",
    "path_to_training_tweets = os.path.join(path_to_data, \"train_tweets\")\n",
    "df_train = load_data(path_to_training_tweets)\n",
    "print(f\"Data loaded in {time()-t:.2f} seconds\")\n",
    "print(\"-\"*50+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping tweets...\n",
      "Tweets grouped in 0.16 seconds\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Group_train = 200\n",
    "print(\"Grouping tweets...\")\n",
    "t = time()\n",
    "df_train_bis = concat_tweets(df_train, MAX_SUBGROUP=Group_train, event_type=True)\n",
    "print(f\"Tweets grouped in {time()-t:.2f} seconds\")\n",
    "print(\"-\"*50+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_embeddings_in_batches(\n",
    "    model=model_Bertweet,\n",
    "    tokenizer=tokenizer,\n",
    "    tweets=df_train_bis[\"Tweet\"].tolist(),\n",
    "    batch_size=10,  # Taille du lot (à ajuster selon votre mémoire)\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"  # GPU si disponible\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropouts):\n",
    "        super().__init__()\n",
    "        assert len(hidden_dims) == len(dropouts)\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_dim = input_dim\n",
    "        for hidden_dim, dropout in zip(hidden_dims, dropouts):\n",
    "            self.layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "        self.output_layer = nn.Linear(in_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, optimizer, criterion, device, num_epochs=10, scheduler=None, weight_1_0=0.5):\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        # Scheduler\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                predicted = (outputs > weight_1_0).float()\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        val_accuracy = correct / total\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['val_loss'].append(val_loss / len(test_loader))\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss/len(test_loader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_bis[\"tweets_embedded\"] = embeddings.tolist()\n",
    "train, test = train_test_split(df_train_bis, test_size=0.01, random_state=42)\n",
    "X_train = train[\"tweets_embedded\"]\n",
    "X_test = test['tweets_embedded']\n",
    "X_train = np.vstack(X_train.tolist())  # Convertir les listes en tableau 2D\n",
    "X_test = np.vstack(X_test.tolist())   # Même opération pour test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sets(train, test, train_X, test_X, batch_size = 64):\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_X)\n",
    "    train_X = scaler.transform(train_X)\n",
    "    test_X = scaler.transform(test_X)\n",
    "    # Labels\n",
    "    train_y = np.array(train['EventType']).reshape(-1, 1).flatten()\n",
    "    test_y = np.array(test['EventType']).reshape(-1, 1).flatten()\n",
    "    train_y = torch.tensor(train_y, dtype=torch.float32).view(-1, 1)\n",
    "    test_y = torch.tensor(test_y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    trainset = TensorDataset(torch.tensor(train_X, dtype=torch.float32), train_y)\n",
    "    testset = TensorDataset(torch.tensor(test_X, dtype=torch.float32), test_y)\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, scaler\n",
    "\n",
    "train_loader, test_loader, scaler = create_sets(train, test, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "output_dim = 1\n",
    "embedding_dim = 2048\n",
    "input_dim = embedding_dim\n",
    "weight_1_0 = 0.55\n",
    "hidden_dims = [embedding_dim*3,embedding_dim//20, embedding_dim//10]\n",
    "dropouts = [0.95] * len(hidden_dims)\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = NNModel(input_dim, hidden_dims, output_dim, dropouts)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/20, Train Loss: 0.8030, Val Loss: 0.6440, Val Accuracy: 0.6402\n",
      "Epoch 2/20, Train Loss: 0.6720, Val Loss: 0.6398, Val Accuracy: 0.6455\n",
      "Epoch 3/20, Train Loss: 0.6664, Val Loss: 0.6372, Val Accuracy: 0.6455\n",
      "Epoch 4/20, Train Loss: 0.6645, Val Loss: 0.6349, Val Accuracy: 0.6455\n",
      "Epoch 5/20, Train Loss: 0.6622, Val Loss: 0.6292, Val Accuracy: 0.6455\n",
      "Epoch 6/20, Train Loss: 0.6602, Val Loss: 0.6297, Val Accuracy: 0.6455\n",
      "Epoch 7/20, Train Loss: 0.6614, Val Loss: 0.6244, Val Accuracy: 0.6455\n",
      "Epoch 8/20, Train Loss: 0.6595, Val Loss: 0.6239, Val Accuracy: 0.6455\n",
      "Epoch 9/20, Train Loss: 0.6563, Val Loss: 0.6207, Val Accuracy: 0.6455\n",
      "Epoch 10/20, Train Loss: 0.6543, Val Loss: 0.6162, Val Accuracy: 0.6455\n",
      "Epoch 11/20, Train Loss: 0.6522, Val Loss: 0.6131, Val Accuracy: 0.6455\n",
      "Epoch 12/20, Train Loss: 0.6506, Val Loss: 0.6103, Val Accuracy: 0.6508\n",
      "Epoch 13/20, Train Loss: 0.6488, Val Loss: 0.6027, Val Accuracy: 0.6614\n",
      "Epoch 14/20, Train Loss: 0.6460, Val Loss: 0.5895, Val Accuracy: 0.6720\n",
      "Epoch 15/20, Train Loss: 0.6418, Val Loss: 0.5829, Val Accuracy: 0.7249\n",
      "Epoch 16/20, Train Loss: 0.6399, Val Loss: 0.5812, Val Accuracy: 0.6825\n",
      "Epoch 17/20, Train Loss: 0.6395, Val Loss: 0.5837, Val Accuracy: 0.6772\n",
      "Epoch 18/20, Train Loss: 0.6341, Val Loss: 0.5731, Val Accuracy: 0.7143\n",
      "Epoch 19/20, Train Loss: 0.6296, Val Loss: 0.5710, Val Accuracy: 0.7249\n",
      "Epoch 20/20, Train Loss: 0.6321, Val Loss: 0.5615, Val Accuracy: 0.7302\n",
      "{'train_loss': [0.8029994037869859, 0.6719591031744055, 0.666434584108934, 0.6645159698920707, 0.6621592747022028, 0.6601747035571973, 0.66139795212713, 0.6595087055474111, 0.6563332176780048, 0.6542856223779182, 0.6521533664366971, 0.6505603141164127, 0.6487547579693468, 0.6459563668460062, 0.6418461547526595, 0.6398973150612557, 0.6394952384576406, 0.6340632234534173, 0.6295799292521934, 0.6320699961626366], 'val_loss': [0.643983801205953, 0.6397999127705892, 0.6372106472651163, 0.6348513762156168, 0.6292238434155782, 0.6297051509221395, 0.6243883967399597, 0.6238652666409811, 0.6207027435302734, 0.6161540548006693, 0.6131433049837748, 0.6102919578552246, 0.6027176380157471, 0.5894876718521118, 0.5828889012336731, 0.5812458594640096, 0.5836500525474548, 0.573093314965566, 0.5710084835688273, 0.5614912311236063], 'val_accuracy': [0.6402116402116402, 0.6455026455026455, 0.6455026455026455, 0.6455026455026455, 0.6455026455026455, 0.6455026455026455, 0.6455026455026455, 0.6455026455026455, 0.6455026455026455, 0.6455026455026455, 0.6455026455026455, 0.6507936507936508, 0.6613756613756614, 0.671957671957672, 0.7248677248677249, 0.6825396825396826, 0.6772486772486772, 0.7142857142857143, 0.7248677248677249, 0.7301587301587301]}\n",
      "Model trained in 27.74 seconds\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print(\"Training the model...\")\n",
    "t = time()\n",
    "print(train_model(model, train_loader, test_loader, optimizer, criterion, device, num_epochs=epochs, weight_1_0=weight_1_0, scheduler=scheduler))\n",
    "print(f\"Model trained in {time()-t:.2f} seconds\")\n",
    "print(\"-\"*50+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with other models (SVM, LG, RF, XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on SVM with regularization:\n"
     ]
    }
   ],
   "source": [
    "Y_train = train['EventType']\n",
    "Y_test = test['EventType']\n",
    "# Regularized SVM\n",
    "print(\"Testing on SVM with regularization:\")\n",
    "t = time()\n",
    "svm = SVC(C=0.1, kernel='linear')  # Smaller C increases regularization\n",
    "svm.fit(X_train, Y_train)\n",
    "print(\"SVM Accuracy:\", svm.score(X_test, Y_test))\n",
    "print(f\"Model trained in {time()-t:.2f} seconds\")\n",
    "\n",
    "# Regularized Logistic Regression\n",
    "print(\"Testing on Logistic Regression with regularization:\")\n",
    "t = time()\n",
    "lg = LogisticRegression(max_iter=1000, C=0.1, penalty='l2', solver='liblinear')  # Stronger L2 regularization\n",
    "lg.fit(X_train, Y_train)\n",
    "print(\"Logistic Regression Accuracy:\", lg.score(X_test, Y_test))\n",
    "print(f\"Model trained in {time()-t:.2f} seconds\")\n",
    "\n",
    "# Regularized Random Forest\n",
    "print(\"Testing on Random Forest with regularization:\")\n",
    "t = time()\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=3)  # Depth/leaf regularization\n",
    "rf.fit(X_train, Y_train)\n",
    "print(\"Random Forest Accuracy:\", rf.score(X_test, Y_test))\n",
    "print(f\"Model trained in {time()-t:.2f} seconds\")\n",
    "\n",
    "# Regularized XGBoost\n",
    "print(\"Testing on XGBoost with regularization:\")\n",
    "t = time()\n",
    "xgb = XGBClassifier(eval_metric='logloss', reg_alpha=1, reg_lambda=1, max_depth=6, learning_rate=0.05, n_estimators=200)\n",
    "xgb.fit(X_train, Y_train)\n",
    "print(\"XGBoost Accuracy:\", xgb.score(X_test, Y_test))\n",
    "print(f\"Model trained in {time()-t:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Loading data...\n",
      "Data loaded in 0.67 seconds\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Loading data...\")\n",
    "t = time()\n",
    "path_to_data = \"../challenge_data/\"\n",
    "path_to_eval_tweets = os.path.join(path_to_data, \"eval_tweets\")\n",
    "df_eval = load_data(path_to_eval_tweets)\n",
    "print(f\"Data loaded in {time()-t:.2f} seconds\")\n",
    "print(\"-\"*50+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping tweets...\n",
      "Tweets grouped in 3.54 seconds\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping tweets\n",
    "Group_eval = 2000\n",
    "print(\"Grouping tweets...\")\n",
    "t = time()\n",
    "df_eval_bis = concat_tweets(df_eval, MAX_SUBGROUP=Group_eval, event_type=False)\n",
    "print(f\"Tweets grouped in {time()-t:.2f} seconds\")\n",
    "print(\"-\"*50+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_eval = get_embeddings_in_batches(\n",
    "    model=model_Bertweet,\n",
    "    tokenizer=tokenizer,\n",
    "    tweets=df_eval_bis[\"Tweet\"].tolist(),\n",
    "    batch_size=10,  # Taille du lot (à ajuster selon votre mémoire)\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"  # GPU si disponible\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_eval_bis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweets_embedded\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43membeddings_eval\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      2\u001b[0m X_eval \u001b[38;5;241m=\u001b[39m df_eval_bis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweets_embedded\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m X_eval \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(X_eval\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings_eval' is not defined"
     ]
    }
   ],
   "source": [
    "df_eval_bis[\"tweets_embedded\"] = embeddings_eval.tolist()\n",
    "X_eval = df_eval_bis[\"tweets_embedded\"]\n",
    "X_eval = np.vstack(X_eval.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     eval_loader \u001b[38;5;241m=\u001b[39m DataLoader(evalset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_loader\n\u001b[0;32m---> 11\u001b[0m eval_loader \u001b[38;5;241m=\u001b[39m create_sets_for_eval(\u001b[43mX_eval\u001b[49m, scaler)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_eval' is not defined"
     ]
    }
   ],
   "source": [
    "def create_sets_for_eval(eval_X, scaler, batch_size = 64):\n",
    "    # Scale the data\n",
    "    eval_X = scaler.transform(eval_X)\n",
    "\n",
    "    evalset = TensorDataset(torch.tensor(eval_X, dtype=torch.float32))\n",
    "\n",
    "    eval_loader = DataLoader(evalset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return eval_loader\n",
    "\n",
    "eval_loader = create_sets_for_eval(X_eval, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_NN = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions_NN.extend(outputs.cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for other models (SVM, LG, RF, XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pridictions_lg = lg.predict(X_eval)\n",
    "pridictions_svm = svm.predict(X_eval)\n",
    "pridictions_rf = rf.predict(X_eval)\n",
    "pridictions_xgb = xgb.predict(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping evaluations for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ID/PeriodID/MatchID and average the predictions and sort by matchID and then PeriodID\n",
    "predictions_eval = predictions_eval.groupby(['ID', 'MatchID', 'PeriodID']).mean().reset_index().sort_values(['MatchID', 'PeriodID'])\n",
    "predictions_eval['Predicted_NN_three'] = (predictions_eval['Predicted_NN'] > weight_1_0).astype(int)\n",
    "predictions_eval['Predicted_LG_three'] = (predictions_eval['Predicted_LG'] > 0.5).astype(int)\n",
    "predictions_eval['Predicted_SVM_three'] = (predictions_eval['Predicted_SVM'] > 0.5).astype(int)\n",
    "predictions_eval['Predicted_RF_three'] = (predictions_eval['Predicted_RF'] > 0.5).astype(int)\n",
    "predictions_eval['Predicted_XGB_three'] = (predictions_eval['Predicted_XGB'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the predictions and the true values as before\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.hist(predictions_eval['Predicted_NN'], bins=20, alpha=0.5, label='NN')\n",
    "plt.axvline(weight_1_0, color='r', linestyle='dotted', label='Threshold')\n",
    "plt.legend()\n",
    "plt.title(f'NN Predictions')\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.hist(predictions_eval['Predicted_LG'], bins=20, alpha=0.5, label='LG')\n",
    "plt.axvline(0.5, color='r', linestyle='dotted', label='Threshold')\n",
    "plt.legend()\n",
    "plt.title(f'LG Predictions')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.hist(predictions_eval['Predicted_SVM'], bins=20, alpha=0.5, label='SVM')\n",
    "plt.axvline(0.5, color='r', linestyle='dotted', label='Threshold')\n",
    "plt.legend()\n",
    "plt.title(f'SVM Predictions')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.hist(predictions_eval['Predicted_RF'], bins=20, alpha=0.5, label='RF')\n",
    "plt.axvline(0.5, color='r', linestyle='dotted', label='Threshold')\n",
    "plt.legend()\n",
    "plt.title(f'RF Predictions')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.hist(predictions_eval['Predicted_XGB'], bins=20, alpha=0.5, label='XGB')\n",
    "plt.axvline(0.5, color='r', linestyle='dotted', label='Threshold')\n",
    "plt.legend()\n",
    "plt.title(f'XGB Predictions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf554",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
