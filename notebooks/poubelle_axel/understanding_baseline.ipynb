{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding `NTLK`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-b/2022/axel.delaval/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/eleves-b/2022/axel.delaval/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download some NLP models for processing, optional\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stopwords \n",
    "\n",
    "*Les stopwords sont des mots communs (comme \"the\", \"is\", etc.) que l'on peut enlever dans des t√¢ches de NLP pour simplifier le texte.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with stopwords removed: example sentence demonstrate removal stopwords.\n"
     ]
    }
   ],
   "source": [
    "# Exemple de suppression des stopwords\n",
    "sample_text = \"This is an example sentence to demonstrate the removal of stopwords.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in sample_text.lower().split() if word not in stop_words]\n",
    "print(\"Text with stopwords removed:\", ' '.join(filtered_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lemmatisation\n",
    "*R√©duit les mots √† une forme plus simple*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['wolf', 'phenomenon', 'running', 'jump', 'easily', 'fairly']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"wolves\", \"phenomena\", \"running\", \"jumps\", \"easily\", \"fairly\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding `Gloves`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GloVe, qui signifie Global Vectors for Word Representation, est un mod√®le de vecteurs de mots d√©velopp√© par l'√©quipe de Stanford. Son objectif est de capturer les relations s√©mantiques entre les mots en cr√©ant des repr√©sentations vectorielles o√π des mots ayant des significations similaires sont proches dans l'espace vectoriel.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le mod√®le GloVe (200 dimensions)\n",
    "embeddings_model = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Look at the similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'football' and 'soccer': 0.84885186\n",
      "Similarity between 'football' and 'basketball': 0.79156566\n",
      "Similarity between 'football' and 'cat': 0.33143294\n",
      "Similarity between 'cat' and 'dog': 0.83243024\n",
      "Similarity between 'cat' and 'puppy': 0.7023193\n",
      "Similarity between 'dog' and 'puppy': 0.7890411\n"
     ]
    }
   ],
   "source": [
    "# Similarit√© entre deux mots\n",
    "word1 = \"football\"\n",
    "word2 = \"soccer\"\n",
    "word3 = \"basketball\"\n",
    "word4 = 'cat'\n",
    "word5 = 'dog'\n",
    "word6 = 'puppy'\n",
    "similarity_12 = embeddings_model.similarity(word1, word2)\n",
    "similarity_13 = embeddings_model.similarity(word1, word3)\n",
    "similarity_14 = embeddings_model.similarity(word1, word4)\n",
    "similarity_45 = embeddings_model.similarity(word4, word5)\n",
    "similarity_46 = embeddings_model.similarity(word4, word6)\n",
    "similarity_56 = embeddings_model.similarity(word5, word6)\n",
    "print(f\"Similarity between '{word1}' and '{word2}':\", similarity_12)\n",
    "print(f\"Similarity between '{word1}' and '{word3}':\", similarity_13)\n",
    "print(f\"Similarity between '{word1}' and '{word4}':\", similarity_14)\n",
    "print(f\"Similarity between '{word4}' and '{word5}':\", similarity_45)\n",
    "print(f\"Similarity between '{word4}' and '{word6}':\", similarity_46)\n",
    "print(f\"Similarity between '{word5}' and '{word6}':\", similarity_56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Show some vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'football': [-0.30796, 0.42961, 0.063245, ... , -0.05676, -0.3919, 0.65645]\n",
      "Vector dimension: 200\n"
     ]
    }
   ],
   "source": [
    "def print_only_few_elements(vector, n=3):\n",
    "    return '[' + ', '.join([str(x) for x in vector[:n]]) + ', ... , ' + ', '.join([str(x) for x in vector[-n:]]) + ']'\n",
    "\n",
    "# Exemple avec un mot\n",
    "word = \"football\"\n",
    "if word in embeddings_model:\n",
    "    vector = embeddings_model[word]\n",
    "    ### show the vector but print only the first 3 and last 3 elements\n",
    "    print(f\"Vector for '{word}':\", print_only_few_elements(vector))\n",
    "    print(\"Vector dimension:\", len(vector))\n",
    "else:\n",
    "    print(f\"'{word}' not found in the model vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. test `get_avg_embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average embedding vector for sentence 'I love watching football games' : [-0.015309997, 0.23268776, -0.05199425, ... , 0.020067502, -0.17745501, 0.31927276]\n"
     ]
    }
   ],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    \"\"\" \n",
    "    Compute the average embedding vector for a tweet\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (str): The tweet text\n",
    "        model (gensim.models.keyedvectors.Word2VecKeyedVectors): The word embeddings model\n",
    "        vector_size (int): The size of the word embedding vectors\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The average embedding vector for the tweet\n",
    "    \"\"\"\n",
    "\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Test avec une phrase\n",
    "sentence = \"I love watching football games\"\n",
    "avg_vector = get_avg_embedding(sentence, embeddings_model, vector_size=200)\n",
    "print(f\"Average embedding vector for sentence '{sentence}' :\", print_only_few_elements(avg_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before preprocessing: 'I love watching football games! #Sport :) üèà'\n",
      "Text after preprocessing: 'love watching football game sport'\n"
     ]
    }
   ],
   "source": [
    "# Basic preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Test de la fonction de pr√©traitement\n",
    "text = \"I love watching football games! #Sport :) üèà\"\n",
    "clean_text = preprocess_text(text)\n",
    "print(f\"Text before preprocessing: '{text}'\")\n",
    "print(f\"Text after preprocessing: '{clean_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*paths* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../../challenge_data/\"\n",
    "\n",
    "path_to_training_tweets = path_to_data + \"train_tweets\"\n",
    "path_to_eval_tweets = path_to_data + \"eval_tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*import the training data into a panda format*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0           2_0        2         0          0  1403538600000   \n",
      "1           2_0        2         0          0  1403538600000   \n",
      "2           2_0        2         0          0  1403538600000   \n",
      "3           2_0        2         0          0  1403538600000   \n",
      "4           2_0        2         0          0  1403538600000   \n",
      "...         ...      ...       ...        ...            ...   \n",
      "5056045  17_129       17       129          1  1403805600000   \n",
      "5056046  17_129       17       129          1  1403805600000   \n",
      "5056047  17_129       17       129          1  1403805600000   \n",
      "5056048  17_129       17       129          1  1403805600000   \n",
      "5056049  17_129       17       129          1  1403805600000   \n",
      "\n",
      "                                                     Tweet  \n",
      "0        RT @soccerdotcom: If #ESP beats #AUS we'll giv...  \n",
      "1        Visit the #SITEP official web site here http:/...  \n",
      "2        RT @soccerdotcom: If #ESP beats #AUS we'll giv...  \n",
      "3        RT @worldsoccershop: If there is a winner in t...  \n",
      "4        RT @soccerdotcom: If #AUS beats #ESP we'll giv...  \n",
      "...                                                    ...  \n",
      "5056045  RT @BBCSport: Portugal fourth team in top 10 o...  \n",
      "5056046  RT @NBCSports: USA MOVES ON! Germany beats #US...  \n",
      "5056047  Ronaldo could have easily scored 4-5 goals ton...  \n",
      "5056048  RT @TheSelenatorBoy: Ppl getting mad bc Pepe i...  \n",
      "5056049  We grew game after game so we won this one. Al...  \n",
      "\n",
      "[5056050 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(path_to_training_tweets):\n",
    "    df = pd.read_csv(path_to_training_tweets + \"/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "# li is the list of dataframes, df is the concatenated dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prepocess by eleminating punctuation, prepositions, etc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0           2_0        2         0          0  1403538600000   \n",
      "1           2_0        2         0          0  1403538600000   \n",
      "2           2_0        2         0          0  1403538600000   \n",
      "3           2_0        2         0          0  1403538600000   \n",
      "4           2_0        2         0          0  1403538600000   \n",
      "...         ...      ...       ...        ...            ...   \n",
      "5056045  17_129       17       129          1  1403805600000   \n",
      "5056046  17_129       17       129          1  1403805600000   \n",
      "5056047  17_129       17       129          1  1403805600000   \n",
      "5056048  17_129       17       129          1  1403805600000   \n",
      "5056049  17_129       17       129          1  1403805600000   \n",
      "\n",
      "                                                     Tweet  \n",
      "0        rt soccerdotcom esp beat au well give away spa...  \n",
      "1        visit sitep official web site httptcoehzkslan ...  \n",
      "2        rt soccerdotcom esp beat au well give away spa...  \n",
      "3        rt worldsoccershop winner au v esp match well ...  \n",
      "4        rt soccerdotcom au beat esp well give away aus...  \n",
      "...                                                    ...  \n",
      "5056045  rt bbcsport portugal fourth team top fifa worl...  \n",
      "5056046  rt nbcsports usa move germany beat usmnt portu...  \n",
      "5056047  ronaldo could easily scored goal tonight finis...  \n",
      "5056048  rt theselenatorboy ppl getting mad bc pepe bra...  \n",
      "5056049  grew game game one always proud portugal matte...  \n",
      "\n",
      "[5056050 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to each tweet\n",
    "df['Tweet'] = df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Transform words into vectors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each tweet and obtain vectors\n",
    "vector_size = 200  # Adjust based on the chosen GloVe model\n",
    "tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df['Tweet']])\n",
    "tweet_df = pd.DataFrame(tweet_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now create X,Y so that we are looking for f s.t. f(X)=Y*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_df], axis=1)\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
    "# We extract the labels of our training samples\n",
    "y = period_features['EventType'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Splitting the data between training and test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Predicting with Logistic Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:  0.7320872274143302\n"
     ]
    }
   ],
   "source": [
    "# We set up a basic classifier that we train and then calculate the accuracy on our test set\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predicting the evaluation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Re-evaluating the predictor on the whole dataset*\n",
    "\n",
    "*By the way, we define a dummy classifier, which constantly predicts the most frequent label that\n",
    "appears in the training set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we train our classifier on the full dataset that it is available to us.\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X, y)\n",
    "# We add a dummy classifier for sanity purposes\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now apply these models on the evaluation dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "dummy_predictions = []\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in os.listdir(path_to_eval_tweets):\n",
    "    val_df = pd.read_csv(path_to_eval_tweets + \"/\" + fname)\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text) # Preprocess the tweets to have the same format as the training data\n",
    "    tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in val_df['Tweet']])\n",
    "    tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "    period_features = pd.concat([val_df, tweet_df], axis=1)\n",
    "    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "    X = period_features.drop(columns=['MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "    preds = clf.predict(X)\n",
    "    dummy_preds = dummy_clf.predict(X)\n",
    "\n",
    "    period_features['EventType'] = preds\n",
    "    period_features['DummyEventType'] = dummy_preds\n",
    "\n",
    "    predictions.append(period_features[['ID', 'EventType']])\n",
    "    dummy_predictions.append(period_features[['ID', 'DummyEventType']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now save them into csv files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('logistic_predictions.csv', index=False)\n",
    "\n",
    "pred_df = pd.concat(dummy_predictions)\n",
    "pred_df.to_csv('dummy_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf554",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
