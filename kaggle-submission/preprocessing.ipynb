{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchtext.data import get_tokenizer\n",
    "from time import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from autocorrect import Speller\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emot \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "emot_obj = emot.core.emot() \n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spell=Speller(lang=\"en\", fast=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "path_to_data = \"../challenge_data/\"\n",
    "path_to_training_tweets = os.path.join(path_to_data, \"train_tweets\")\n",
    "path_to_eval_tweets = os.path.join(path_to_data, \"eval_tweets\")\n",
    "output_path = \"evaluation_predictions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data(path, verbose=False):\n",
    "    li = []\n",
    "    for filename in os.listdir(path):\n",
    "        df = pd.read_csv(os.path.join(path, filename))\n",
    "        li.append(df)\n",
    "    output = pd.concat(li)\n",
    "    if verbose:\n",
    "        print(output.head())\n",
    "        print(f'The shape of the data is: {output.shape}')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "list_of_countries_trigrams = [\n",
    "    'AFG', 'RSA', 'ALB', 'ALG', 'GER', 'AND', 'ENG', 'ANG', 'AIA', 'ATG', 'KSA', 'ARG', 'ARM', 'ARU', 'AUS', 'AUT',\n",
    "    'AZE', 'BAH', 'BHR', 'BAN', 'BRB', 'BEL', 'BLZ', 'BEN', 'BER', 'BHU', 'BLR', 'MYA', 'BOL', 'BIH', 'BOT', 'BRA',\n",
    "    'BRU', 'BUL', 'BFA', 'BDI', 'CAM', 'CMR', 'CAN', 'CPV', 'CHI', 'CHN', 'CYP', 'COL', 'COM', 'CGO', 'PRK', 'KOR',\n",
    "    'CRC', 'CIV', 'CRO', 'CUB', 'CUR', 'DEN', 'DJI', 'DMA', 'SCO', 'EGY', 'UAE', 'ECU', 'ERI', 'ESP', 'EST', 'ESW',\n",
    "    'USA', 'ETH', 'FIJ', 'FIN', 'FRA', 'GAB', 'GAM', 'GEO', 'GHA', 'GIB', 'GRE', 'GRN', 'GUA', 'GUM', 'GUI', 'EQG',\n",
    "    'GNB', 'GUY', 'HAI', 'HON', 'HKG', 'HUN', 'CAY', 'COK', 'FRO', 'SOL', 'TCA', 'VGB', 'VIR', 'IND', 'IDN', 'IRQ',\n",
    "    'IRN', 'IRL', 'NIR', 'ISL', 'ISR', 'ITA', 'JAM', 'JPN', 'JOR', 'KAZ', 'KEN', 'KGZ', 'KVX', 'KUW', 'LAO', 'LES',\n",
    "    'LVA', 'LBN', 'LBR', 'LBY', 'LIE', 'LTU', 'LUX', 'MAC', 'MKD', 'MAD', 'MAS', 'MWI', 'MDV', 'MLI', 'MLT', 'MAR',\n",
    "    'MRI', 'MTN', 'MEX', 'MDA', 'MNG', 'MNE', 'MSR', 'MOZ', 'NAM', 'NEP', 'NCA', 'NIG', 'NGA', 'NOR', 'NCL', 'NZL',\n",
    "    'OMA', 'UGA', 'UZB', 'PAK', 'PLE', 'PAN', 'PNG', 'PAR', 'NED', 'WAL', 'PER', 'PHI', 'POL', 'PUR', 'POR', 'QAT',\n",
    "    'COD', 'CTA', 'DOM', 'CZE', 'ROU', 'RUS', 'RWA', 'SKN', 'SMR', 'VIN', 'LCA', 'SLV', 'SAM', 'ASA', 'STP', 'SEN',\n",
    "    'SRB', 'SEY', 'SLE', 'SIN', 'SVK', 'SVN', 'SOM', 'SDN', 'SSD', 'SRI', 'SWE', 'SUI', 'SUR', 'SYR', 'TJK', 'TAH',\n",
    "    'TPE', 'TAN', 'CHA', 'THA', 'TLS', 'TOG', 'TGA', 'TRI', 'TUN', 'TKM', 'TUR', 'UKR', 'URU', 'VAN', 'VEN', 'VIE',\n",
    "    'YEM', 'ZAM', 'ZIM', 'BOE', 'GUF', 'GBR', 'GLP', 'NMI', 'KIR', 'MTQ', 'NIU', 'REU', 'SMN', 'SMA', 'TUV', 'ZAN',\n",
    "    'ALA', 'COR', 'GRL', 'GUE', 'IMA', 'FLK', 'MHL', 'JER', 'MYT', 'FSM', 'MCO', 'PLW', 'EUS', 'ESH', 'BLM', 'SPM',\n",
    "    'SHN', 'VAT', 'WLF'\n",
    "]\n",
    "\n",
    "list_of_countries_full_names = english_countries = ['Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Congo', 'Congo (Democratic Republic)', 'Costa Rica', 'Croatia', 'Cuba', 'Cyprus', 'Czechia', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'East Timor', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini', 'Ethiopia', 'Fiji', 'Finland', 'France', 'Gabon', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Ivory Coast', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kosovo', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Mauritania', 'Mauritius', 'Mexico', 'Federated States of Micronesia', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar (Burma)', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'North Korea', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'St Kitts and Nevis', 'St Lucia', 'St Vincent', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Korea', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Sweden', 'Switzerland', 'Syria', 'Tajikistan', 'Tanzania', 'Thailand', 'The Bahamas', 'The Gambia', 'Togo', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Vatican City', 'Venezuela', 'Vietnam', 'Yemen', 'Zambia', 'Zimbabwe']\n",
    "\n",
    "\n",
    "# Combine trigrams and full names into one list\n",
    "list_of_countries_trigrams.extend(list_of_countries_full_names)\n",
    "list_of_countries_trigrams = list(set(list_of_countries_trigrams))  # Remove duplicates\n",
    "\n",
    "# Preload stopwords (ensure NLTK stopwords are downloaded)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "country_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, list_of_countries_trigrams)) + r\")\\b\", re.IGNORECASE)\n",
    "WORD = re.compile(r'\\w+')\n",
    "def reTokenize(doc):\n",
    "    tokens = WORD.findall(doc)\n",
    "    return tokens\n",
    "\n",
    "def transform_emoticons_to_text(text):\n",
    "    emoticons = emot_obj.emoticons(text)\n",
    "    text_modified = text \n",
    "    for i in range(len(emoticons['value'])):\n",
    "        text_modified = text_modified.replace(emoticons['value'][i], emoticons['mean'][i])\n",
    "    return text_modified\n",
    "\n",
    "def preprocessing(df):\n",
    "    # Compile regex patterns for efficiency\n",
    "    url_pattern = re.compile(r\"http[s]?://\\S+|www\\.\\S+\")\n",
    "    mention_pattern = re.compile(r\"@\\w+\")\n",
    "    hashtag_pattern = re.compile(r\"#\\w+\")\n",
    "    number_pattern = re.compile(r\"\\d+\")\n",
    "    punctuation_pattern = re.compile(r\"[^\\w\\s]\")\n",
    "    country_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, list_of_countries_trigrams)) + r\")\\b\", re.IGNORECASE)\n",
    "    rt_pattern = re.compile(r\"rt\")\n",
    "\n",
    "    def clean_tweet(tweet):\n",
    "        tweet = tweet.lower()  # Lowercase the text\n",
    "        tweet = transform_emoticons_to_text(tweet)  # Replace emoticons with text\n",
    "        tweet = url_pattern.sub(\"\", tweet)  # Remove URLs\n",
    "        tweet = rt_pattern.sub(\"\", tweet)  # Remove RT\n",
    "        tweet = mention_pattern.sub(\"\", tweet)  # Replace mentions with 'user'\n",
    "        tweet = hashtag_pattern.sub(\"\", tweet)  # Replace hashtags with 'hashtag'\n",
    "        tweet = number_pattern.sub(\"\", tweet)  # Replace numbers with 'number'\n",
    "        tweet = country_pattern.sub(\"\", tweet)  # Replace country trigrams with 'country'\n",
    "        tweet = punctuation_pattern.sub(\"\", tweet)  # Remove punctuation\n",
    "        tweet = \" \".join([word for word in tweet.split() if word not in stop_words])  # Remove stopwords\n",
    "        words = tweet.split()\n",
    "        words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words if word not in stop_words]  # Lemmatize\n",
    "        words = [stemmer.stem(word) for word in words if word not in stop_words]  # Remove stopwords and apply stemming\n",
    "        tweet = \" \".join(words)\n",
    "        tweet = ' '.join([spell(w) for w in reTokenize(tweet)]) \n",
    "        return tweet\n",
    "\n",
    "    # Apply cleaning to the \"Tweet\" column\n",
    "    df[\"Tweet\"] = df[\"Tweet\"].apply(clean_tweet)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and save in a csv file\n",
    "train_tweets = load_data(path_to_training_tweets)\n",
    "eval_tweets = load_data(path_to_eval_tweets)\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "train_tweets = preprocessing(train_tweets)\n",
    "eval_tweets = preprocessing(eval_tweets)\n",
    "\n",
    "# Save \n",
    "train_tweets.to_csv(\"df_train_tweets_with_emot.csv\", index=False)\n",
    "eval_tweets.to_csv(\"df_eval_tweets_with_emot.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf554bis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
